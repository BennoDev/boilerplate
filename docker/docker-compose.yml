# https://medium.com/@blackhorseya/deploying-opentelemetry-collector-jaeger-and-prometheus-with-docker-compose-for-observability-fedd7c0898b5
# https://www.npmjs.com/package/@opentelemetry/auto-instrumentations-node
# https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/README.md
# https://opentelemetry.io/docs/collector/configuration/
# https://hub.docker.com/r/grafana/grafana
# https://hub.docker.com/r/prom/prometheus
# https://grafana.com/docs/loki/latest/setup/install/docker/
# https://github.com/grafana/intro-to-mltp/blob/main/docker-compose-otel.yml

# OTEL INSTRUMENTATION:
# Instrumentation needs to happen at the earliest possible point. Ideally before any framework / libraries are initialized.
# This is why we have a separate init.ts file for each service currently, but ideally you can export an /init file from a library or something and import that.
# If not instrumenting early, half of the instrumentations will not work at all.
#
# BULLMQ:
# Adding OTEL support to BullMQ is not difficult; you need to use the bullmq-otel package & BullMQOtel class.
# IMPORTANT: Leaving the above in for clarity, however the bullmq-otel package is flawed.
# Instead, use the @appsignal/opentelemetry-instrumentation-bullmq package for a more
# standard approach, that seems to handle errors in the job better.
# For NestJS, we are once again dealing with confusing BullMQ setup, as we have:
#  - forRoot(Async)
#  - registerQueue(Async)
#  - @Processor
# The place where you add this is important.
# For the PRODUCER, you CAN add it at the forRoot level - and MAYBE at the registerQueue level if you want to have multiple queues with different span names - which makes sense!
# For the CONSUMER, you MUST add it at the @Processor level. Any other place seems to have no effect whatsoever!
#
# DISTRIBUTED TRACING:
# For distributed tracing, you NEED to have propagation through a textMapPropagator!
# What the purpose of the SpanProcessor is - not entirely clear. Both with and without processor it works quite well :)
# Tracing with custom decorators, or via the { trace } import from @opentelemetry/api is straightforward and powerful.
# Errors in tracing:
#     When they occur in the sync part, aka the immediate request, they are nicely shown in the trace.
#     When they occur in the async part, aka the job, they are not shown in the trace. Even with BullMQ instrumentation.
#
# LOGGING: https://grafana.com/docs/loki/latest/send-data/otel/
# OTEL tracing makes own tracing solution redundant.
# HTTP OTEL exporter seems to be the best solution for logging, not sure why.
#
# GRAFANA LGMT:
# The LGTM stack is Loki, Grafana, Tempo & (Mimir/Prometheus) for logging, dashboard, tracing & metrics respectively.
# There is a comprehensive docker image that includes all of these and instruments with OTEL: https://hub.docker.com/r/grafana/otel-lgtm
#
# GRAFANA TEMPO + LOKI
# tracesToLogs is super useful for easily navigating and not having to manually query logs based on span / trace id.
# By using "derivedFields" in Grafana, you can easily add a "View Trace" link to the logs, essentially adding logs2Traces.
# Check datasources.yml for how to configure this.
#
# METRICS:
# Combining prom-client with the OTEL Metrics Exporter seems to be non-trivial currently.
# We'll have to rely on the metrics from the basic exporter / reader for now.
# This means that metrics to traces won't be possible as we don't really have HTTP metrics currently.
# https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/#trace-to-metrics
# Metrics to traces could be a cool feature, but is nowhere near as useful as traces to logs <-> logs to traces.
#
# OTEL SAMPLING:
# Because we are using an OTEL collector, we can potentially use that as a lever to sample / adapt volumes.
# https://opentelemetry.io/docs/concepts/sampling/
#
# GRAFANA ALLOY:
# Alloy seems complicated compared to the OTEL collector.
# The documentation and existing examples are not very helpful.
# It probably has a lot of features, but this is really digging into the infrastructure side.
# Applications are not affected by this, developer experience could be, as Alloy
# may enable features such as Profile graphs and Service graphs.
# In a real, larger production environment that embraces the Grafana stack,
# this is probably the way to go.
#
# GRAFANA BEYLA: https://grafana.com/oss/beyla-ebpf/
# Beyla is a tool that uses "eBPF" to trace applications down to
# kernel level interactions, such as HTTP requests. It does this without
# the need for instrumentation from the application. It's quite powerful,
# but will still not replace application-level instrumentation as tracing
# is still best with code-level instrumentation.
#
# TODO:
# - Check out full LGTM project: https://github.com/grafana/intro-to-mltp/tree/main
# - Less important: check out Grafana Alloy: https://grafana.com/docs/alloy/latest/set-up/install/docker/ & https://medium.com/@magstherdev/grafana-alloy-opentelemetry-59c171d2ebfc
#   - Set up Service Graph if possible
# - Less important: check out Grafana Pyroscope https://grafana.com/docs/pyroscope/latest/get-started/
#   - Easier with Alloy
#   - Investigate profiles to traces
#   - Doesn't go through OTEL collector, no instrumentation for Node ?yet?.
# - Less important: check out Grafana Mimir?
#   - Prometheus "replacement"
# - Less important: check out Grafana Faro? https://github.com/grafana/faro-web-sdk/

services:
    postgres:
        # Replace :latest with desired version when starting a new project
        image: postgres:latest
        ports:
            - 5432:5432
        env_file:
            - ./.env.postgres
    redis:
        # Replace :latest with desired version when starting a new project
        image: redis:latest
        ports:
            - 6379:6379

    otel-collector:
        image: otel/opentelemetry-collector:latest
        command: ['--config=/etc/otel-collector-config.yml']
        ports:
            - 4317:4317
        volumes:
            - ./config/otel-collector-config.yml:/etc/otel-collector-config.yml

    # Check UI at http://localhost:9090
    prometheus:
        image: prom/prometheus:latest
        ports:
            - 9090:9090
        volumes:
            - ./config/prometheus.yml:/etc/prometheus.yml

    tempo:
        image: grafana/tempo:latest
        command: ['-config.file=/etc/tempo/tempo-config.yml']
        ports:
            - 3200:3200
        volumes:
            - ./config/tempo-config.yml:/etc/tempo/tempo-config.yml

    loki:
        image: grafana/loki:latest
        ports:
            - 3100:3100
            - 9096:9096
        volumes:
            - ./config/loki-config.yml:/etc/loki/loki-config.yaml

    # Check UI at http://localhost:3000
    grafana:
        image: grafana/grafana:latest
        ports:
            - 3000:3000
        environment:
            - GF_SECURITY_ADMIN_PASSWORD=admin
        volumes:
            - ./config/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
            - ./volumes/grafana:/var/lib/grafana
